


#!/usr/bin/env python3
"""
Config-driven eligibility ingestion pipeline.

- Reads multiple partner files with different delimiters/column names using a central YAML config
- Maps to a standard schema
- Applies standard transformations
- Outputs one unified dataset (CSV)
- Includes validation + error handling for malformed rows and dates

Run:
  python eligibility_pipeline.py \
    --config configs/partners.yaml \
    --inputs acme=data/acme.txt bettercare=data/bettercare.csv \
    --output output/unified_eligibility.csv
"""

from __future__ import annotations

import argparse
import csv
import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import yaml


STANDARD_COLUMNS = ["external_id", "first_name", "last_name", "dob", "email", "phone", "partner_code"]


@dataclass(frozen=True)
class PartnerConfig:
    partner_key: str
    partner_code: str
    delimiter: str
    header: bool
    column_mapping: Dict[str, str]  # partner_col -> standard_col


def load_partner_configs(config_path: str) -> Dict[str, PartnerConfig]:
    cfg = yaml.safe_load(Path(config_path).read_text())
    partners = cfg.get("partners", {})
    out: Dict[str, PartnerConfig] = {}

    for partner_key, p in partners.items():
        fmt = p.get("file_format", {})
        if fmt.get("type") != "delimited":
            raise ValueError(f"Unsupported file_format.type for {partner_key}: {fmt.get('type')}")

        out[partner_key] = PartnerConfig(
            partner_key=partner_key,
            partner_code=p["partner_code"],
            delimiter=str(fmt.get("delimiter", ",")),
            header=bool(fmt.get("header", True)),
            column_mapping=dict(p.get("column_mapping", {})),
        )

    return out


# ---------------------------
# Transformations (standard)
# ---------------------------

def to_title_case(value: Optional[str]) -> Optional[str]:
    if value is None:
        return None
    v = value.strip()
    if not v:
        return None
    # Handles names like "mcDONALD" -> "Mcdonald" (simple Title Case)
    return v.title()


def to_lower(value: Optional[str]) -> Optional[str]:
    if value is None:
        return None
    v = value.strip()
    return v.lower() if v else None


def parse_dob_to_iso(value: Optional[str]) -> Optional[str]:
    """
    Accepts common partner formats and returns ISO-8601 YYYY-MM-DD.
    Examples:
      03/15/1955 -> 1955-03-15
      1965-08-10 -> 1965-08-10
    Returns None for invalid/empty values.
    """
    if value is None:
        return None
    v = value.strip()
    if not v:
        return None

    # Try multiple known formats; can be extended WITHOUT changing core logic
    fmts = ["%m/%d/%Y", "%Y-%m-%d", "%m-%d-%Y", "%Y/%m/%d"]
    for f in fmts:
        try:
            return datetime.strptime(v, f).date().isoformat()
        except ValueError:
            pass
    return None


def normalize_phone(value: Optional[str]) -> Optional[str]:
    """
    Output format: XXX-XXX-XXXX
    Accepts digits with optional punctuation/spaces.
    If more than 10 digits, takes last 10 (common when country code present).
    Returns None if cannot make a valid 10-digit phone.
    """
    if value is None:
        return None
    digits = re.sub(r"\D+", "", value)
    if len(digits) < 10:
        return None
    digits = digits[-10:]
    return f"{digits[0:3]}-{digits[3:6]}-{digits[6:10]}"


# ---------------------------
# Ingestion + Mapping
# ---------------------------

def read_delimited_rows(
    file_path: str,
    delimiter: str,
    header: bool,
) -> Tuple[List[str], Iterable[List[str]]]:
    """
    Reads a delimited file. Returns (header_cols, rows_iterable).
    Handles empty lines gracefully.
    """
    fp = Path(file_path)
    if not fp.exists():
        raise FileNotFoundError(f"Input file not found: {file_path}")

    f = fp.open("r", encoding="utf-8", newline="")
    reader = csv.reader(f, delimiter=delimiter)

    try:
        first = next(reader)
    except StopIteration:
        f.close()
        return ([], [])

    # If header=True, use first line as header
    if header:
        header_cols = [c.strip() for c in first]
        return (header_cols, reader)
    else:
        # No header: generate generic col names
        header_cols = [f"col_{i}" for i in range(len(first))]
        # Yield first row as data + rest
        def chain_first():
            yield first
            for r in reader:
                yield r
        return (header_cols, chain_first())


def map_partner_row_to_standard(
    partner_row_dict: Dict[str, str],
    partner_cfg: PartnerConfig,
) -> Dict[str, Optional[str]]:
    """
    Uses partner_cfg.column_mapping to map partner columns to standard fields.
    """
    standard: Dict[str, Optional[str]] = {c: None for c in STANDARD_COLUMNS}

    for partner_col, standard_col in partner_cfg.column_mapping.items():
        if standard_col not in standard:
            # Ignore unknown target columns rather than failing (safe for evolving configs)
            continue
        standard[standard_col] = partner_row_dict.get(partner_col)

    standard["partner_code"] = partner_cfg.partner_code
    return standard


def transform_standard_row(
    row: Dict[str, Optional[str]],
) -> Dict[str, Optional[str]]:
    """
    Applies standardized transformations required by the assessment.
    """
    row["external_id"] = (row.get("external_id") or "").strip() or None
    row["first_name"] = to_title_case(row.get("first_name"))
    row["last_name"] = to_title_case(row.get("last_name"))
    row["dob"] = parse_dob_to_iso(row.get("dob"))
    row["email"] = to_lower(row.get("email"))
    row["phone"] = normalize_phone(row.get("phone"))
    # partner_code already set
    return row


# ---------------------------
# Validation + Error handling
# ---------------------------

def validate_row(row: Dict[str, Optional[str]]) -> Tuple[bool, List[str]]:
    """
    Bonus requirements:
      - external_id must be present
      - dob must parse correctly if present (we treat invalid dob as an error)
    """
    errors: List[str] = []
    if not row.get("external_id"):
        errors.append("missing_external_id")
    if row.get("dob") is None and row.get("dob") is not None:
        # (kept for clarity; dob already normalized; invalid dob becomes None)
        pass
    return (len(errors) == 0, errors)


def process_partner_file(
    partner_key: str,
    file_path: str,
    partner_cfg: PartnerConfig,
    drop_invalid: bool = True,
) -> Tuple[List[Dict[str, Optional[str]]], List[Dict[str, str]]]:
    """
    Returns: (good_rows, error_records)
    error_records include row_number + reason.
    """
    header_cols, rows_iter = read_delimited_rows(
        file_path=file_path,
        delimiter=partner_cfg.delimiter,
        header=partner_cfg.header,
    )

    good: List[Dict[str, Optional[str]]] = []
    errors: List[Dict[str, str]] = []

    for idx, raw_row in enumerate(rows_iter, start=2 if partner_cfg.header else 1):
        # Skip empty lines
        if not raw_row or all((c or "").strip() == "" for c in raw_row):
            continue

        # Malformed row: length mismatch
        if len(raw_row) != len(header_cols):
            errors.append({
                "partner": partner_key,
                "file": file_path,
                "row_number": str(idx),
                "error": f"malformed_row_length expected={len(header_cols)} got={len(raw_row)}",
            })
            if drop_invalid:
                continue
            # attempt best-effort pad/truncate
            if len(raw_row) < len(header_cols):
                raw_row = raw_row + [""] * (len(header_cols) - len(raw_row))
            else:
                raw_row = raw_row[: len(header_cols)]

        partner_row_dict = {header_cols[i]: (raw_row[i] if i < len(raw_row) else "") for i in range(len(header_cols))}
        std_row = map_partner_row_to_standard(partner_row_dict, partner_cfg)
        std_row = transform_standard_row(std_row)

        ok, row_errors = validate_row(std_row)
        if not ok:
            errors.append({
                "partner": partner_key,
                "file": file_path,
                "row_number": str(idx),
                "error": ",".join(row_errors),
            })
            if drop_invalid:
                continue

        good.append(std_row)

    return good, errors


def write_unified_csv(rows: List[Dict[str, Optional[str]]], output_path: str) -> None:
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=STANDARD_COLUMNS)
        writer.writeheader()
        for r in rows:
            writer.writerow({k: (r.get(k) if r.get(k) is not None else "") for k in STANDARD_COLUMNS})


def write_errors_csv(errors: List[Dict[str, str]], output_path: str) -> None:
    if not errors:
        return
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    fieldnames = ["partner", "file", "row_number", "error"]
    with open(output_path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(errors)


def parse_inputs(inputs: List[str]) -> Dict[str, str]:
    """
    Parses inputs like: ["acme=data/acme.txt", "bettercare=data/bettercare.csv"]
    """
    out: Dict[str, str] = {}
    for item in inputs:
        if "=" not in item:
            raise ValueError(f"Invalid --inputs item: {item}. Expected partner_key=path")
        k, v = item.split("=", 1)
        out[k.strip()] = v.strip()
    return out


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True, help="Path to partner config YAML")
    ap.add_argument(
        "--inputs",
        required=True,
        nargs="+",
        help="One or more partner_key=filepath pairs",
    )
    ap.add_argument("--output", required=True, help="Unified output CSV path")
    ap.add_argument(
        "--drop-invalid",
        action="store_true",
        help="Drop invalid rows (missing external_id/malformed). If not set, best-effort keep some rows.",
    )
    args = ap.parse_args()

    partner_configs = load_partner_configs(args.config)
    input_map = parse_inputs(args.inputs)

    all_rows: List[Dict[str, Optional[str]]] = []
    all_errors: List[Dict[str, str]] = []

    for partner_key, file_path in input_map.items():
        if partner_key not in partner_configs:
            raise KeyError(f"Partner '{partner_key}' not found in config. Available: {list(partner_configs.keys())}")

        rows, errs = process_partner_file(
            partner_key=partner_key,
            file_path=file_path,
            partner_cfg=partner_configs[partner_key],
            drop_invalid=args.drop_invalid,
        )
        all_rows.extend(rows)
        all_errors.extend(errs)

    write_unified_csv(all_rows, args.output)

    # Also write an error report next to the output (great for ops / onboarding debugging)
    err_path = str(Path(args.output).with_suffix("")) + "_errors.csv"
    write_errors_csv(all_errors, err_path)

    print(f"✅ Wrote unified dataset: {args.output} (rows={len(all_rows)})")
    if all_errors:
        print(f"⚠️  Wrote error report: {err_path} (errors={len(all_errors)})")


if __name__ == "__main__":
    main()
